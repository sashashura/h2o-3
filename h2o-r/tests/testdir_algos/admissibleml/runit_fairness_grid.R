setwd(normalizePath(dirname(R.utils::commandArgs(asValues=TRUE)$"f")))
source("../../../scripts/h2o-r-test-setup.R")


.calculate_confusion_matrix <-
  function(predicted, actual, favorable_class = "1") {
    t <- predicted == actual
    f <- predicted != actual

    p <- predicted == favorable_class
    n <- predicted != favorable_class

    tp <- sum(t & p)
    fp <- sum(f & p)
    tn <- sum(t & n)
    fn <- sum(f & n)

    total <- tp + fp + tn + fn

    return(
      list(
        TP = tp,
        FP = fp,
        TN = tn,
        FN = fn,
        Accuracy = (tp + tn) / total,
        Precision = tp / (tp + fp),
        Sensitivity = tp / (tp + fn),
        Specificity = tn / (fp + tn),
        F1 = (2 * tp) / (2 * tp + fp + fn),
        FalsePositiveRate = fp / (fp + tn),
        FalseNegativeRate = fn / (fn + tp),
        Selected = tp + fp,
        SelectedRatio = (tp + fp) / total,
        Total = total
      )
    )
  }

.get_group_mask <- function(group, newdata) {
  mask <- rep_len(TRUE, nrow(newdata))

  for (col in names(group)) {
    if (!is.na(group[[col]])) {
      mask <- mask & (newdata[[col]] == group[[col]])
    }
  }

  return(mask)
}


h2o.calculate_disparate_measures <-
  function(model,
           newdata,
           sensitive_features,
           reference_groups = list(),
           criterion = "AUTO",
           favorable_class = "0",
           thresholds = list(air = c(0.8, 1.25),
                             p.value = 0.05,
                             smd = c(-0.8, 0.8)),
           minimum_reference_group_factor = 0.8,
           normalize_air = FALSE) {
    if (class(model) == "H2OAutoML") {
      y <- model@leader@allparameters$y
    } else {
      y <- model@allparameters$y
    }

    is_classification <- is.factor(newdata[[y]])

    if (missing(criterion) || criterion == "AUTO") {
      if (is_classification) {
        criterion <- "SelectedRatio"
      } else {
        criterion <- "meanPrediction"
      }
    }

    newdata_df <- as.data.frame(newdata)
    predictions <- as.data.frame(predict(model, newdata)$predict)

    results <- expand.grid(lapply(sensitive_features, function(col)
      unlist(c(
        NA, levels(newdata_df[[col]])
      ))),
                           stringsAsFactors = FALSE)
    names(results) <- sensitive_features

    results <- tail(results, n = -1)
    row.names(results) <- seq_len(nrow(results))

    results$reference <- NA
    mask_cache <- list()

    for (idx in seq_len(nrow(results))) {
      mask <- rep_len(1, nrow(predictions))
      name <- NULL
      for (col in seq_len(ncol(results[idx,]))) {
        if (!is.na(results[idx, col])) {
          col <- names(results)[col]
          val <- results[idx, col][[1]]
          name <- c(name, paste0(col, "=", val))
          cname <- paste0(name, collapse = ",")
          if (!cname %in% names(mask_cache)) {
            mask_cache[[cname]] <- mask & (newdata_df[[col]] == val)
          }
          mask <- mask_cache[[cname]]
        }
      }
      if (is_classification) {
        cm <-
          .calculate_confusion_matrix(predictions[mask, "predict"], newdata_df[mask, y],
                                      favorable_class = favorable_class)
        results[idx, "TP"] <- cm$TP
        results[idx, "FP"] <- cm$FP
        results[idx, "TN"] <- cm$TN
        results[idx, "FN"] <- cm$FN

        results[idx, "Accuracy"] <- cm$Accuracy
        results[idx, "Precision"] <- cm$Precision
        results[idx, "Sensitivity"] <- cm$Sensitivity
        results[idx, "Specificity"] <- cm$Specificity
        results[idx, "F1"] <- cm$F1
        results[idx, "Total"] <- cm$Total
        results[idx, "Selected"] <- cm$Selected
        results[idx, "SelectedRatio"] <- cm$SelectedRatio
      } else {
        results[idx, "MSE"] <-
          mean((predictions[mask, "predict"] - newdata_df[mask, y]) ** 2)
        results[idx, "RMSE"] <- sqrt(results[idx, "MSE"])
        results[idx, "MAE"] <-
          mean(abs(predictions[mask, "predict"] - newdata_df[mask, y]))
        results[idx, "meanPrediction"] <-
          mean(predictions[mask, "predict"])
      }
    }
    groups <-
      combn(c(rep_len(NA, length(sensitive_features) - 1), sensitive_features), length(sensitive_features))

    # Get reference
    for (gid in seq_len(ncol(groups))) {
      cols <- sensitive_features[sensitive_features %in% groups[, gid]]
      fix_to_na <- (sensitive_features[!sensitive_features %in% groups[, gid]])

      group_mask <- Reduce("&", Map(function(col) {
        if (col %in% fix_to_na) {
          is.na(results[col])
        } else {
          !is.na(results[col])
        }
      }, sensitive_features))

      references_mask <-
        group_mask & Reduce("&", Map(function(col) {
          if (is.null(reference_groups[[col]])) {
            TRUE
          } else {
            results[col] == reference_groups[[col]]
          }
        }, cols))

      refs <- results[references_mask, ]

      if (!is.na(minimum_reference_group_factor)) {
        refs <- refs[refs$Total > sum(refs$Total) / nrow(refs) * minimum_reference_group_factor, ]
      }

      results[group_mask, "reference"] <-
        row.names(refs[which.max(refs[[criterion]]), ])
    }
    if (is_classification) {
      # Calculate air & me
      for (row_id in row.names(results)) {
        ref <- results[row_id, "reference"]
        air <- results[row_id, criterion] / results[ref, criterion]
        if (normalize_air && isTRUE(air > 1)) air <- 1/air
        results[row_id, "air"] <- air
        results[row_id, "me"] <- results[ref, criterion] - results[row_id, criterion]

        m <-
          matrix(
            c(results[row_id, "Selected"], results[row_id, "Total"] - results[row_id, "Selected"],
              results[ref, "Selected"], results[ref, "Total"] - results[ref, "Selected"]),
            nrow = 2,
            dimnames = list(c("Selected", "Not Selected"), c("Protected", "Reference"))
          )
        ft <- fisher.test(m, alternative = "t")
        results[row_id, "p.value"] <- ft$p.value
      }

      results[["adverse_impact"]] <- FALSE

      if (!is.null(thresholds$air))
        results[["adverse_impact"]] <-
          results[["adverse_impact"]] |
            results[["air"]] < thresholds$air[[1]] |
            results[["air"]] > thresholds$air[[2]]
      if (!is.null(thresholds$me))
        results[["adverse_impact"]] <-
          results[["adverse_impact"]] |
            results[["me"]] < thresholds$me[[1]] |
            results[["me"]] > thresholds$me[[2]]
      if (!is.null(thresholds$p.value))
        results[["adverse_impact"]] <-
          results[["adverse_impact"]] &
            results[["p.value"]] < thresholds$p.value[[1]]
    } else {
      # Calculate smd
      sd_prediction <- sd(predictions$predict)
      for (row_id in row.names(results)) {
        ref <- results[row_id, "reference"]
        results[row_id, "smd"] <-
          (results[row_id, criterion] - results[ref, criterion]) / sd_prediction

        current_group_mask <-
          .get_group_mask(results[row_id, sensitive_features], newdata_df)
        reference_mask <-
          .get_group_mask(results[ref, sensitive_features], newdata_df)

        results[row_id, "p.value"] <-
          wilcox.test(predictions$predict[current_group_mask],
                      predictions$predict[reference_mask])$p.value
      }

      results[["adverse_impact"]] <- FALSE

      if (!is.null(thresholds$smd))
        results[["adverse_impact"]] <-
          results[["smd"]] < thresholds$smd[[1]] |
            results[["smd"]] > thresholds$smd[[2]]
      if (!is.null(thresholds$p.value))
        results[["adverse_impact"]] <-
          results[["adverse_impact"]] &
            results[["p.value"]] > thresholds$p.value[[1]]
    }
    return(results)
  }

fairness_metrics_are_correct_test <- function() {

  train <- h2o.importFile("https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_train.csv")
  test <- h2o.importFile("https://raw.githubusercontent.com/h2oai/article-information-2019/master/data/output/hmda_test.csv")

  train <- train[!is.na(train$black) & !is.na(train$hispanic) & !is.na(train$male) & !is.na(train$agegte62),]
  test <- test[!is.na(test$black) & !is.na(test$hispanic) & !is.na(test$male) & !is.na(test$agegte62),]

  train$high_priced <- as.factor(train$high_priced)
  test$high_priced <- as.factor(test$high_priced)

  train$ethnic <- "NA"
  train[train$black == 1, "ethnic"] <- "black"
  train[train$asian == 1, "ethnic"] <- "asian"
  train[train$white == 1, "ethnic"] <- "white"
  train[train$amind == 1, "ethnic"] <- "amind"
  train[train$hipac == 1, "ethnic"] <- "hipac"
  train[train$hispanic == 1, "ethnic"] <- "hispanic"
  train$sex <- "NA"
  train[train$female, "sex"] <- "F"
  train[train$male, "sex"] <- "M"
  train$ethnic <- as.factor(train$ethnic)
  train$sex <- as.factor(train$sex)

  test$ethnic <- "NA"
  test[test$black == 1, "ethnic"] <- "black"
  test[test$asian == 1, "ethnic"] <- "asian"
  test[test$white == 1, "ethnic"] <- "white"
  test[test$amind == 1, "ethnic"] <- "amind"
  test[test$hipac == 1, "ethnic"] <- "hipac"
  test[test$hispanic == 1, "ethnic"] <- "hispanic"
  test$sex <- "NA"
  test[test$female, "sex"] <- "F"
  test[test$male, "sex"] <- "M"
  test$ethnic <- as.factor(test$ethnic)
  test$sex <- as.factor(test$sex)

  x <- c("loan_amount", "loan_to_value_ratio", "no_intro_rate_period", "intro_rate_period", "property_value", "income", "debt_to_income_ratio", "term_360", "conforming")
  y <- "high_priced"
  aml <- h2o.automl(x, y, training_frame = train, max_models = 12)

  for (model_id in as.character(as.list(aml@leaderboard$model_id))) {
    m <- h2o.getModel(model_id)
    for (fav_class in c("0", "1")) {
      java_metrics <- h2o.calculate_fairness_metrics(m, test, protected_cols = c("ethnic", "sex"), reference = c("white", "M"), favorable_class = fav_class)$overview
      R_metrics <- h2o.calculate_disparate_measures(m, test, sensitive_features = c("ethnic", "sex"), favorable_class = fav_class, reference_groups = c(ethnic = "white", sex = "M"))
      names(java_metrics) <- tolower(names(java_metrics))
      java_metrics$air <- java_metrics$air_selectedratio
      names(R_metrics) <- tolower(names(R_metrics))
      cols <- intersect(names(java_metrics), names(R_metrics))
      merged <- merge(java_metrics, R_metrics, by = c("ethnic", "sex"), suffixes = c("_java", "_r"))
      for (col in cols) {
        if (col %in% c("ethnic", "sex")) next;
        expect_equal(merged[, paste0(col, "_java")], merged[, paste0(col, "_r")])
      }
    }
  }
}

doTest("Test that fairness metrics are correctly computed", fairness_metrics_are_correct_test)
